# -*- coding: utf-8 -*-
"""
Agent-based GraphRAG (final, multilingual-first, graph-bounded + vector hybrid)
- Multilingual keyword normalization by LLM (no synonym map)
- Cypher is generated by LLM with injected normalized TERMS (READ-ONLY)
- Subgraph snapshot includes labels and intra-matched relations
- Evidence = graph-bounded nodes only, ranked by vector (if available) else keyword overlap
- Compose strictly grounds on [Subgraph Snapshot] + [Evidence Passages]
"""

import os
import uuid
import time
import json
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

from dotenv import load_dotenv
load_dotenv()

# Pydantic alias (avoid collision with user's BaseModel)
from pydantic import BaseModel as PydanticBaseModel, Field

# LangChain / Neo4j
from langchain_community.graphs import Neo4jGraph
from langchain_community.vectorstores.neo4j_vector import Neo4jVector
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain.tools import tool
try:
    from langchain.tools import StructuredTool
except ImportError:
    from langchain_core.tools import StructuredTool

# GraphCypherQAChain (version-safe import)
try:
    from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain
except ImportError:
    try:
        from langchain.chains import GraphCypherQAChain
    except ImportError:
        GraphCypherQAChain = None

EXPERIMENT_MODE = os.getenv("EXPERIMENT_MODE", "graph_only").strip()
print("LangChain 1.0 detected - using a simplified manual tool chain")
AGENT_TYPE = "simple"
AgentExecutor = None


class ProperLangChainGraphRAG:
    """
    Compatible with the user's Evaluator expectations:
      - initialize(self) -> bool
      - answer_question(self, question: str) -> Dict[str, Any]
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.model_name = "ProperLangChainGraphRAG"
        self.config = config or {}
        self.response_times: List[float] = []

        # Neo4j connection
        self.neo4j_url = os.getenv("NEO4J_URI", "bolt://localhost:7687")
        self.neo4j_username = os.getenv("NEO4J_USERNAME", "neo4j")
        self.neo4j_password = os.getenv("NEO4J_PASSWORD", "12345678")

        # OpenAI
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

        # LangChain components
        self.neo4j_graph: Optional[Neo4jGraph] = None
        self.llm: Optional[ChatOpenAI] = None
        self.cypher_chain: Optional[GraphCypherQAChain] = None
        self.embeddings: Optional[OpenAIEmbeddings] = None
        self.vector_store: Optional[Neo4jVector] = None

        # Tools
        self.tools = []
        self.agent = None
        self.agent_executor: Optional[AgentExecutor] = None

    # ---------- initialization ----------
    def initialize(self) -> bool:
        try:
            print("Initialization started.")

            # 1) Neo4j
            print("Connecting Neo4jGraph...")
            self.neo4j_graph = Neo4jGraph(
                url=self.neo4j_url,
                username=self.neo4j_username,
                password=self.neo4j_password,
                refresh_schema=False
            )
            try:
                self.neo4j_graph.refresh_schema()
                print("Schema loaded.")
            except Exception as e:
                print(f"Schema auto-load failed: {e}")
                # Fallback schema aligned to user's real KG (no uuid/doc/EVIDENCE_FOR)
                self.neo4j_graph.schema = """
                Node properties:
                  HudongItem {title: STRING, detail: STRING, url: STRING, image: STRING,
                              openTypeList: STRING, baseInfoKeyList: STRING, baseInfoValueList: STRING}
                  NewNode {title: STRING}
                  Weather {title: STRING}
                Relationship properties:
                  RELATION {type: STRING}
                  Weather2Plant {type: STRING}
                  CityWeather {type: STRING}
                Relations include:
                  (HudongItem)-[:RELATION]->(HudongItem/NewNode),
                  (Weather)-[:Weather2Plant]->(HudongItem),
                  (HudongItem/NewNode)-[:CityWeather]->(Weather)
                """

            # 2) LLM
            print("Initializing ChatOpenAI...")
            self.llm = ChatOpenAI(
                model=self.openai_model,
                temperature=0,
                openai_api_key=self.openai_api_key,
                max_retries=2,
                timeout=60,
            )

            # 3) Cypher chain with READ-ONLY guard
            if GraphCypherQAChain is not None:
                print("Initializing GraphCypherQAChain...")
                cypher_guard = PromptTemplate(
                    input_variables=["schema", "question"],
                    template=(
                        "You are a Neo4j Cypher expert for an Agricultural Knowledge Graph (Chinese/English/Korean titles).\n"
                        "Generate exactly ONE READ-ONLY Cypher query using the SCHEMA below.\n"
                        "FORBIDDEN: CREATE, MERGE, DELETE, SET, CALL apoc.*\n"
                        "ALLOWED: MATCH, OPTIONAL MATCH, WHERE, RETURN only. Always include LIMIT 50.\n"
                        "- Match nodes by title (primary identifier).\n"
                        "- Use partial matching with CONTAINS for the TERMS provided by the user hint.\n"
                        "- Avoid exact equality unless obvious; prefer OR over multiple CONTAINS.\n\n"
                        "CRITICAL PROPERTY NAME:\n"
                        "- The property for node content is 'detail' (SINGULAR), NOT 'details' (plural)\n"
                        "- WRONG: RETURN n.title, n.details\n"
                        "- CORRECT: RETURN n.title, n.detail\n"
                        "- You MUST use 'n.detail' in your RETURN clause\n\n"
                        "[SCHEMA]\n{schema}\n\n[QUESTION]\n{question}\n"
                        "Output only the Cypher query with RETURN n.title, n.detail LIMIT 50"
                    ),
                )
                self.cypher_chain = GraphCypherQAChain.from_llm(
                    llm=self.llm,
                    graph=self.neo4j_graph,
                    verbose=False,
                    validate_cypher=True,
                    return_intermediate_steps=True,
                    top_k=10,
                    cypher_prompt=cypher_guard
                )
            else:
                print("GraphCypherQAChain unavailable; falling back to a minimal manual query.")
                self.cypher_chain = None

            # 4) Embeddings for vector ranking (optional)
            print("Initializing OpenAI embeddings...")
            self.embeddings = OpenAIEmbeddings(
                model=os.getenv("OPENAI_EMBED_MODEL", "text-embedding-3-small"),
                openai_api_key=self.openai_api_key
            )

            # 5) Neo4jVector (optional; used for hybrid ranking if available)
            try:
                print("Initializing Neo4jVector...")
                self.vector_store = Neo4jVector.from_existing_graph(
                    embedding=self.embeddings,
                    url=self.neo4j_url,
                    username=self.neo4j_username,
                    password=self.neo4j_password,
                    index_name=os.getenv("NEO4J_VECTOR_INDEX", "agricultural_vectors"),
                    node_label=os.getenv("NEO4J_VECTOR_NODE_LABEL", "HudongItem"),
                    text_node_properties=json.loads(os.getenv("NEO4J_VECTOR_TEXT_PROPS", '["title","detail"]')),
                    embedding_node_property=os.getenv("NEO4J_VECTOR_EMB_PROP", "embedding"),
                )
                try:
                    _ = self.vector_store.similarity_search("ping", k=1)
                    print("Neo4jVector ready.")
                except Exception as e:
                    print(f"Vector health check failed: {e}")
            except Exception as e:
                print(f"Neo4jVector init failed: {e}")
                self.vector_store = None

            # 6) Tools
            self._setup_tools()

            # 7) Manual chain (no agent executor)
            self._create_agent()
            print("Initialization finished.")
            return True

        except Exception as e:
            print(f"Initialization failed: {e}")
            return False

    # ---------- multilingual normalization (LLM-only, no synonym map) ----------
    def _multilingual_normalize(self, question: str, max_terms: int = 12) -> List[str]:
        """
        Normalize the question into ko/zh/en keyword candidates that are likely to match KG node titles.
        Returns a JSON list of strings. No synonym map or env flags are used.
        """
        prompt = (
            "Normalize the following question into up to 12 concise keywords/phrases "
            "that are most likely to match knowledge graph node titles. "
            "Return a JSON list of strings only (no commentary). "
            "Prefer common surface forms in Chinese (简体), English, and Korean.\n\n"
            f"Question: {question}"
        )
        try:
            res = self.llm.invoke(prompt).content.strip()
            data = json.loads(res)
            if isinstance(data, list):
                out, seen = [], set()
                for s in data:
                    if not isinstance(s, str):
                        continue
                    t = s.strip()
                    if not t or len(t) > 40:
                        continue
                    lt = t.lower()
                    if lt not in seen:
                        out.append(t)
                        seen.add(lt)
                return out[:max_terms]
            return []
        except Exception:
            # Fallback: crude whitespace token fallback
            toks = [t for t in question.replace(",", " ").split() if t and len(t) <= 40]
            out, seen = [], set()
            for t in toks:
                lt = t.lower()
                if lt not in seen:
                    out.append(t); seen.add(lt)
            return out[:max_terms]

    # ---------- subgraph snapshot helpers ----------
    def _snapshot_top_labels(self, node_titles: List[str], limit: int = 6) -> List[str]:
        if not node_titles:
            return []
        q = """
        MATCH (n)
        WHERE n.title IN $titles
        UNWIND labels(n) AS l
        RETURN l AS label, count(*) AS cnt
        ORDER BY cnt DESC
        LIMIT $L
        """
        rows = self.neo4j_graph.query(q, {"titles": node_titles, "L": limit})
        return [f"{row['label']} x{row['cnt']}" for row in (rows or [])]

    def _snapshot_top_relations(self, node_titles: List[str], limit: int = 10) -> List[str]:
        if not node_titles:
            return []
        q = """
        MATCH (a)-[r]-(b)
        WHERE a.title IN $titles AND b.title IN $titles
        WITH labels(a) AS la, type(r) AS rt, labels(b) AS lb
        RETURN la AS a_labels, rt AS rel_type, lb AS b_labels, count(*) AS cnt
        ORDER BY cnt DESC
        LIMIT $L
        """
        rows = self.neo4j_graph.query(q, {"titles": node_titles, "L": limit})
        triples = []
        for row in (rows or []):
            a_lbl = "/".join(row.get("a_labels", []))
            b_lbl = "/".join(row.get("b_labels", []))
            triples.append(f"({a_lbl})-[:{row['rel_type']}]->({b_lbl}) x{row['cnt']}")
        return triples

    def _build_subgraph_snapshot(self, node_titles: List[str], sample_n: int = 5) -> str:
        lines = []
        lines.append(f"- matched_nodes: {len(node_titles)}")
        lines.append(f"- sample_nodes: {node_titles[:sample_n]}")
        top_labels = self._snapshot_top_labels(node_titles)
        if top_labels:
            lines.append("- top_labels:")
            for l in top_labels:
                lines.append(f"  - {l}")
        else:
            lines.append("- top_labels: (none)")
        top_rels = self._snapshot_top_relations(node_titles)
        if top_rels:
            lines.append("- top_relations:")
            for t in top_rels:
                lines.append(f"  - {t}")
        else:
            lines.append("- top_relations: (none)")
        return "\n".join(lines)

    # ---------- tools ----------
    def _setup_tools(self):
        """Cypher→JSON (with structural snapshot), Evidence (graph-bounded + vector hybrid), Compose."""

        @tool
        def langchain_cypher_json(question: str) -> str:
            """
            Convert a question to Cypher (READ-ONLY), run it, then return:
              {"cypher": str, "node_titles": [..], "subgraph_summary": str}
            The subgraph_summary includes labels and relations among matched nodes.
            """
            try:
                if self.cypher_chain is None:
                    rows = self.neo4j_graph.query("""
                        MATCH (n) WHERE n.title IS NOT NULL
                        RETURN collect(distinct n.title)[..50] AS node_titles
                    """)
                    node_titles = rows[0].get("node_titles", []) if rows else []
                    subgraph_summary = self._build_subgraph_snapshot(node_titles)
                    return json.dumps(
                        {"cypher": "(manual)", "node_titles": node_titles, "subgraph_summary": subgraph_summary},
                        ensure_ascii=False
                    )

                # 1) multilingual normalization (LLM-only)
                norm_terms = self._multilingual_normalize(question, max_terms=12)
                hint_block = (
                    "HINT: Use partial matching with CONTAINS over n.title for the following normalized terms.\n"
                    f"TERMS: {', '.join(norm_terms)}\n"
                    "Prefer MATCH/OPTIONAL MATCH + WHERE ("
                    + " OR ".join([f"n.title CONTAINS '{t}'" for t in norm_terms]) +
                    ") ."
                )
                cypher_input = {"question": f"{hint_block}\n\nUSER QUESTION: {question}"}

                # 2) generate and run Cypher
                res = self.cypher_chain.invoke(cypher_input)

                cypher_query = ""
                node_titles: List[str] = []
                ctx_rows: List[dict] = []

                for step in res.get("intermediate_steps", []):
                    if isinstance(step, dict):
                        if "query" in step:
                            cypher_query = step["query"]
                        if "context" in step and isinstance(step["context"], list):
                            ctx_rows.extend(step["context"])

                for row in ctx_rows:
                    if not isinstance(row, dict):
                        continue
                    for k, v in row.items():
                        if "title" in str(k).lower() and isinstance(v, str):
                            node_titles.append(v)

                node_titles = list({t for t in node_titles if t})
                subgraph_summary = self._build_subgraph_snapshot(node_titles)

                return json.dumps({
                    "cypher": cypher_query,
                    "node_titles": node_titles,
                    "subgraph_summary": subgraph_summary
                }, ensure_ascii=False)

            except Exception as e:
                return json.dumps({"error": f"Cypher JSON tool error: {str(e)}"}, ensure_ascii=False)

        # Evidence tool (graph-bounded + vector hybrid)
        class EvidenceInput(PydanticBaseModel):
            question: str = Field(..., description="Original question")
            node_titles: List[str] = Field(..., description="Titles matched in the graph")
            k: int = Field(default=24, description="Max candidates to retrieve")
            topn: int = Field(default=8, description="Returned snippet count")

        def _get_node_details(node_titles: List[str], limit=24):
            q = """
            MATCH (n)
            WHERE n.title IN $titles
            OPTIONAL MATCH (n)-[r:RELATION]-(m)
            WHERE m.title IS NOT NULL
            WITH n, r, m
            LIMIT $L
            RETURN n.title AS title,
                   n.detail AS detail,
                   n.url AS url,
                   labels(n) AS labels,
                   collect({neighbor: m.title, rel_type: r.type}) AS neighbors
            """
            return self.neo4j_graph.query(q, {"titles": node_titles, "L": limit})

        def _keyword_overlap_score(text: str, terms: List[str]) -> int:
            t = (text or "").lower()
            score = 0
            for term in terms:
                tt = term.lower()
                if tt and tt in t:
                    score += 1
            return score

        def evidence_search_impl(question: str, node_titles: List[str], k=24, topn=8) -> str:
            """
            Evidence = graph-bounded:
              1) Gather candidate snippets from matched nodes: detail + limited neighbor context
              2) Rank by vector similarity if Neo4jVector supports filter by title; else rank by keyword-overlap
            """
            try:
                if not node_titles:
                    return "[]"

                rows = _get_node_details(node_titles, limit=k)
                if not rows:
                    return "[]"

                # Build candidate snippets from graph
                candidates: List[Dict[str, Any]] = []
                for row in rows:
                    title = row.get("title", "")
                    detail = row.get("detail", "") or ""
                    url = row.get("url", "") or ""
                    labels = row.get("labels", [])
                    neighbors = row.get("neighbors", [])

                    parts = []
                    if detail:
                        parts.append(detail)

                    # add limited neighbor context
                    neighbor_info = []
                    for nb in neighbors[:5]:
                        if nb.get("neighbor") and nb.get("rel_type"):
                            neighbor_info.append(f"{nb['rel_type']}: {nb['neighbor']}")
                    if neighbor_info:
                        parts.append("Related: " + "; ".join(neighbor_info))

                    text = "\n".join(parts) if parts else title
                    candidates.append({
                        "title": title,
                        "text": text,
                        "url": url,
                        "node_type": "/".join(labels) if labels else "Unknown"
                    })

                # Rank: try vector store constrained to node_titles
                ranked: List[Tuple[float, Dict[str, Any]]] = []

                used_vector = False
                if self.vector_store:
                    try:
                        # Attempt vector search filtered by titles (implementation-dependent).
                        # If filter is not supported by your Neo4jVector version, this will raise.
                        vs_docs = self.vector_store.similarity_search_with_score(
                            question, k=min(k, len(node_titles)),
                            filter={"title": {"$in": node_titles}}
                        )
                        # Map by title for scoring (take max score per title)
                        score_by_title: Dict[str, float] = {}
                        for doc, score in vs_docs:
                            t = (doc.metadata or {}).get("title") or doc.page_content[:50]
                            if t:
                                # Lower score = closer (depending on store impl). Normalize to descending relevance.
                                # We convert to a descending "relevance" by negative score.
                                rel = -float(score)
                                if t not in score_by_title or rel > score_by_title[t]:
                                    score_by_title[t] = rel

                        for cand in candidates:
                            rel = score_by_title.get(cand["title"], float("-inf"))
                            ranked.append((rel, cand))
                        ranked.sort(key=lambda x: x[0], reverse=True)
                        used_vector = True
                    except Exception:
                        used_vector = False

                if not used_vector:
                    # Fallback: keyword-overlap ranking using normalized terms
                    terms = self._multilingual_normalize(question, max_terms=12)
                    for cand in candidates:
                        rel = _keyword_overlap_score(cand["text"], terms)
                        ranked.append((float(rel), cand))
                    ranked.sort(key=lambda x: x[0], reverse=True)

                top = [c for _, c in ranked[:topn]]
                return json.dumps(top, ensure_ascii=False)

            except Exception as e:
                return json.dumps({"error": f"Evidence tool error: {str(e)}"}, ensure_ascii=False)

        evidence_search = StructuredTool.from_function(
            func=evidence_search_impl,
            name="evidence_search",
            description="Graph-bounded evidence: detail + neighbors; rank by vector (if possible) else keyword overlap.",
            args_schema=EvidenceInput
        )

        # Compose tool
        class ComposeInput(PydanticBaseModel):
            question: str = Field(..., description="Original question")
            subgraph_summary: str = Field(..., description="Subgraph snapshot (includes labels and relations)")
            passages_json: str = Field(..., description="Evidence snippets as JSON list")
            allow_global_note: bool = Field(default=False, description="If global evidence used, note it explicitly")

        def compose_final_answer_impl(question: str, subgraph_summary: str, passages_json: str, allow_global_note: bool = False) -> str:
            try:
                passages = []
                if passages_json:
                    parsed = json.loads(passages_json)
                    if isinstance(parsed, list):
                        passages = parsed
                ev_lines = []
                for p in passages[:10]:
                    title = p.get("title") or p.get("doc_id", "doc")
                    url = p.get("url", "")
                    txt = p.get("text", "")
                    if len(txt) > 800:
                        txt = txt[:800] + "..."
                    ev_lines.append(f"- ({title}) {url} :: {txt}")

                evidence_block = "\n".join(ev_lines) or "- (no evidence)"

                rules_extra = ""
                if allow_global_note:
                    rules_extra = "\n- If any global evidence is used, annotate it as 'global' and prioritize graph-based evidence."

                prompt = f"""
[Question]
{question}

[Subgraph Snapshot]
{subgraph_summary}

[Evidence Passages]
{evidence_block}

[Rules]
- Use only the provided Subgraph Snapshot and Evidence Passages as grounds.
- End each claim with (source: <title or doc_id>).
- Even if evidence is limited, try to provide an answer based on the available information.
- If the evidence is very limited, acknowledge this in your answer but still provide what information is available.
- Each claim must be one line (one sentence per line).
- Keep it concise with 8–10 sentences.{rules_extra}
"""
                return self.llm.invoke(prompt).content
            except Exception as e:
                return f"[compose error] {str(e)}"

        compose_final_answer = StructuredTool.from_function(
            func=compose_final_answer_impl,
            name="compose_final_answer",
            description="Compose the final answer strictly from subgraph summary + evidence snippets.",
            args_schema=ComposeInput
        )

        # Register tools (global vector search tool omitted by design in graph_only mode)
        self.tools = [
            langchain_cypher_json,
            evidence_search,
            compose_final_answer,
        ]

    # ---------- manual chain ----------
    def _create_agent(self):
        print("Using manual tool chain (no agent executor).")
        self.agent = None
        self.agent_executor = None

    # ---------- run ----------
    def answer_question(self, question: str) -> Dict[str, Any]:
        response_id = str(uuid.uuid4())
        ts = datetime.now().isoformat()
        t0_total = time.time()

        answer_text = ""
        context: Dict[str, Any] = {}
        metadata: Dict[str, Any] = {
            "experiment_mode": EXPERIMENT_MODE,
            "tools_registered": [getattr(t, "name", "tool") for t in self.tools],
            "kg_utilized": False,
            "entities_found": [],
            "total_kg_relations": 0,
            "evidence_doc_ids": [],
            "node_count": 0,
            "evidence_count": 0,
            "has_sufficient_evidence": False,
        }
        kg_retrieval_time = 0.0
        api_response_time = 0.0

        try:
            # 1) cypher→json
            cypher_json_tool = next(t for t in self.tools if getattr(t, "name", "") == "langchain_cypher_json")
            cypher_result_raw = cypher_json_tool.func(question)
            cypher_data = json.loads(cypher_result_raw)
            node_titles: List[str] = cypher_data.get("node_titles", []) or []
            subgraph_summary: str = cypher_data.get("subgraph_summary", "") or ""
            metadata["cypher"] = cypher_data.get("cypher", "")
            metadata["node_titles_sample"] = node_titles[:10]

            # 2) Check if any nodes were found - if not, skip this question
            if not node_titles:
                # No nodes found in KG - skip this question for Graph-RAG
                return None

            t0_kg = time.time()
            evidence_tool = next(t for t in self.tools if getattr(t, "name", "") == "evidence_search")
            passages_json = evidence_tool.func(question=question, node_titles=node_titles, k=24, topn=8)
            kg_retrieval_time = time.time() - t0_kg

            passages = []
            try:
                parsed = json.loads(passages_json)
                if isinstance(parsed, list):
                    passages = parsed
            except:
                passages = []
            evidence_node_titles = [p.get("title") for p in passages if p.get("title")]
            metadata["evidence_node_titles"] = evidence_node_titles

            # Count relations among matched nodes (for logging)
            total_kg_relations = 0
            try:
                total_kg_relations = self._count_relations_safe(node_titles)
            except:
                total_kg_relations = 0
            metadata["total_kg_relations"] = total_kg_relations

            metadata["kg_utilized"] = bool(node_titles)
            metadata["entities_found"] = node_titles
            metadata["node_count"] = len(node_titles)
            metadata["evidence_count"] = len(passages)
            # Determine if evidence is sufficient (threshold: at least 1 passage)
            metadata["has_sufficient_evidence"] = len(passages) >= 1

            # 3) compose - always generate answer if nodes were found, even with limited evidence
            compose_tool = next(t for t in self.tools if getattr(t, "name", "") == "compose_final_answer")

            # Always compose answer when nodes exist
            if True:
                t0_api = time.time()
                answer_text = compose_tool.func(
                    question=question,
                    subgraph_summary=subgraph_summary,
                    passages_json=passages_json,
                    allow_global_note=False
                )
                api_response_time = time.time() - t0_api
                context = {"subgraph_summary": subgraph_summary, "evidence": passages}

            total_time = time.time() - t0_total
            return {
                "id": response_id,
                "timestamp": ts,
                "model_name": self.model_name,
                "question": question,
                "answer": answer_text,
                "context": context,
                "response_time": total_time,
                "kg_retrieval_time": kg_retrieval_time,
                "api_response_time": api_response_time,
                "metadata": metadata
            }

        except Exception as e:
            total_time = time.time() - t0_total
            return {
                "id": response_id,
                "timestamp": ts,
                "model_name": self.model_name,
                "question": question,
                "answer": f"[ERROR] {str(e)}",
                "context": {},
                "response_time": total_time,
                "kg_retrieval_time": kg_retrieval_time,
                "api_response_time": api_response_time,
                "error": True,
                "metadata": metadata
            }

    # --- utility ---
    def _count_relations_safe(self, node_titles: List[str]) -> int:
        q = """
        MATCH (a)-[r]-(b)
        WHERE a.title IN $titles AND b.title IN $titles
        RETURN count(r) AS rels
        """
        try:
            rows = self.neo4j_graph.query(q, {"titles": node_titles})
            return int(rows[0]["rels"]) if rows else 0
        except:
            return 0


# ------------ local test ------------
if __name__ == "__main__":
    model = ProperLangChainGraphRAG()
    if model.initialize():
        tests = [
            "벼 파종 시기는 언제야? (Korean)",
            "When is the planting season for rice? (English)",
            "水稻的播种季节是什么时候？ (Chinese)"
        ]
        print("\n=== Agent-based GraphRAG test ===")
        for q in tests:
            res = model.answer_question(q)
            print(f"\nQ: {q}")
            print(f"A: {res['answer'][:400]}...")
            md = res.get("metadata", {})
            print(f"mode: {md.get('experiment_mode')}, tools: {md.get('tools_registered')}")
            print(f"kg_utilized: {md.get('kg_utilized')}, entities_found: {len(md.get('entities_found', []))}, rels: {md.get('total_kg_relations')}")
            print(f"time: {res['response_time']:.2f}s (kg {res.get('kg_retrieval_time',0):.2f}s, api {res.get('api_response_time',0):.2f}s)")
            print("-" * 80)
    else:
        print("Initialization failed")
